\documentclass[12pt, a4paper]{article}

% Text languages
\usepackage[spanish, english, UKenglish, USenglish, american, british]{babel}

% Accents
\usepackage[latin1]{inputenc}

% Maths
\usepackage{mathtools}
\usepackage{amsmath,amsthm,amssymb}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

% Swap the definition of \abs* and \norm*, so that \abs
% and \norm resizes the size of the brackets, and the 
% starred version does not.
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
%
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother


% https://www.overleaf.com/learn/latex/Page_size_and_margins
\usepackage{geometry}
\topmargin = -23pt
\oddsidemargin = 13pt
\headheight = 12pt
\headsep = 25pt
\textheight = 674pt
\textwidth = 426pt
\marginparsep = 10pt
\marginparwidth = 50pt
\footskip = 30pt
\marginparpush = 5pt
\hoffset = 0pt
\voffset = 0pt
\paperwidth = 597pt
\paperheight = 845pt

% Hyperlinks
\usepackage{hyperref}

% Figure
\usepackage{graphicx}
% \usepackage{subcaption}
\usepackage{etoc}
% Example
\newtheorem{exmp}{Example}[section]
% Algorithms
%\usepackage[]{algorithm2e}
%\usepackage{algorithm}% http://ctan.org/pkg/algorithm
%\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{algpseudocode}

\renewcommand{\thefootnote}{\arabic{footnote}} % 1, 2, 3... (la que hay por defecto)

\usepackage{titlesec}
\setcounter{secnumdepth}{5}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\usepackage{float}
%--------------------------------------------------------------------------
\title{Detection of/between similarity of documents with hashing}
\author{Roger Vilaseca Darné and Xavier Martín Ballesteros\\
  \small Algorithms\\
}
\date{10th December 2018}

\begin{document}
% Images
\graphicspath{ {./images/}, {./plots/} }

%\maketitle

\begin{titlepage}
	\centering
	{\scshape\LARGE UNIVERSITAT POLITÈCNICA DE CATALUNYA \par}
	\vspace{1cm}
	{\scshape\Large ALGORTIHMS\par}
	\vspace{1.5cm}
	{\huge\bfseries Detection of similarity between documents with hashing\par}
	\vspace{2cm}
	{\Large\itshape Roger Vilaseca Darné and Xavier Martín Ballesteros\par}
	\vfill
	\includegraphics[width=0.15\textwidth]{UPC.png}\par\vspace{1cm}
	%supervised by\par
	%Dr.~Mark \textsc{Brown}

	\vfill

% Bottom of the page
	{\large 10th December 2018}
\end{titlepage}

%\abstract{Esto es una plantilla simple para un articulo en \LaTeX.}

%	*********************** ÍNDEX *********************
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}

\newpage
  \tableofcontents
\newpage

\section{Introduction}

% Referència a una equació \ref{eq:area}).
% Referència a una secció \ref{sec:nada}
% Referència a una cita \cite{Cd94}.

In this experiment we are going to create differents algorithms to compute the difference between various documents in order to be able to detect if there may have been a possibility of copy in any pair of the documents.
To do it, we have started with a very basic algorithm that returns the Jaccard Similarity with two documents and we have finished obtaining a Locality-sensitive hashing algorithm that given the paths of several documents will return which documents may have been copied. (All the algorithms are explained in the document).

\section{Jaccard Similarity}
The Jaccard Similarity, also known as Intersection Over Union (IOU), calculates the percentage of similarity between two sets.

For any pair of sets S and T, the Jaccard Similarity is defined as:
% For any pair of sets S and T, we can define the Jaccard Similarity as shown:
\begin{equation}
J(S, T) = \frac{\abs{S \cap T}}{\abs{S \cup T}}
\end{equation}

We can easily deduce that the more common words, the bigger the Jaccard Similarity, which means that it is more probable that one set is a duplicate of the other.

\begin{exmp}
In Figure 1 we see two sets S and T. There are 3 elements in their intersection ("I", "love", "chocolate") and 6 in their union ("I", "love", "chocolate", "and", "pizza", "white"). Thus, J(S, T) = 3/6.

\begin{center}
	\includegraphics[width=3in]{JaccardExample}
	\label{fig:JaccardExample}
	
	Figure 1: Two sets with Jaccard Similarity 3/6.
\end{center}

% Falta dir què passa si hi ha poques paraules? Fals positiu...?

\end{exmp}

\section{Shingling of Documents}

Any pair of documents can be compared by watching the number of repeated strings they have. The more common strings, the more probable is that one is a duplicate from the other.
One way to represent a document as a set is to insert in the set each string that appears in it. If we do so, then duplicated documents that have reorganized the sentences or even the entire text will have plenty of common strings, and will be detected as duplicated.

\subsection{k-Shingles}

The idea is not to insert in the set all the words, but a set of characters of size \textit{k}. Thus, each element of the set will have the same size as the others.

The question now is how big \textit{k} should be? If we take a small value of \textit{k}, this will result in many shingles that are present in all documents. Suppose we choose the extreme case (\textit{k} = 1). Then, all documents would result to be similar, as the most used characters are present in all documents. However, if we take a big value of \textit{k}, then any pair of documents would not share a shingle.

The value of \textit{k} depends on the size of the documents. A poem will not have the same \textit{k} value than an article. Otherwise, we could have the problems mentioned before.
According to Anand Rajaraman, Jure Leskovec,
and Jeffrey D. Ullman (2011), "k should be picked large enough that the probability of any given shingle appearing in any given document is low." (p. 78).


\section{Compairing Similarity Using The Sets}

If we succeed in shingling the documents by using the \textit{k}-Shingling technique, we will only have to compare all pairs of documents using the Jaccard Similarity and say if there is similarity between them or not. 
In order to do this, we have to store all the information in a data structure, for instance, a matrix. By doing this, we have two problems: time and space complexity.

\subsection{Matrix Representation}

To represent the matrix, we will put the documents' sets in the columns and the union of all the documents' sets in the rows. The values of the matrix will be the following:
\[
\begin{dcases}
    1,& \text{if column \textit{c} contains row \textit{r}} \\
    0,              & \text{otherwise}
\end{dcases}
\]

\begin{exmp}
For this example we will use two sets representing the words "Nadal" and "Nadia". Let k = 2 to form the k-Shingles.

\begin{center}
	\includegraphics[width=2in]{matrixRepresentation}
	\label{fig:matrixRepresentation}
	
	Figure 2: Representation of the matrix with two sets S and T.
\end{center}

\label{exmp:matrixRepresentation}
\end{exmp}

% Parlar de que la probabilitat és gairebé la mateixa?

In this type of matrices, for any pair of columns we can have 4 types of results, which are the permutations of 0s and 1s of size 2:

\begin{center}
	\includegraphics[width=2in]{typesValues}
	\label{fig:typesValues}
	
	\textit{Figure 3: Representation of the matrix with all the possible permutations.}
\end{center}

Note that as the matrix is sparse, most of the rows will be of type \textit{a}. If we try to calculate the similarity between two sets $S_1$ and $S_2$ using the matrix and the Jaccard Similarity, we will have the following result:

\begin{equation}
J(S_1, S_2) = \dfrac{Q(d)}{Q(b)+Q(c)+Q(d)}
\end{equation}

Where Q(x) is the number of rows of type \textit{x}. Q(d) is the intersection of the sets and Q(b) + Q(c) + Q(d) is the union of the sets.

\subsubsection{Time Complexity}

Imagine we have \textit{n} documents. Then, we have to compare each document with all the rest. Thus, the number of comparisons we have to do is $n*(n-1)/2$ which is equal to $O$($n^2$).

\begin{exmp}
Suppose we have 1 million documents. The number of comparisons would be $5*10^{11}$ which is a huge number.

\begin{equation}
\dfrac{(1*10^{6})*999.999} {2} = 499.999,5*10^{6} \approx 5*10^{11}
\end{equation}

\end{exmp}

\subsubsection{Space Complexity}

In typical applications the matrix is sparse, which means that there are more 0s than 1s.

If we take \textit{k} shingles, then the document have relatively few of the possible shingles.
Another way to think about this is with the toys in Christmas Day. Kids would be the columns of the matrix and toys, the rows. Usually, kids would like to have a specific toy, which is very popular at that moment. Then, lots of toys would not be buyed for any kid.

\subsection{Minhashing}

The main goal using minhashing is to reduce a lot the space complexity. We can achieve this by subsituting the matrix shown before by another matrix called "signature matrix".

Signatures are smaller representations of the sets, but they still preserve the similarity of the sets they represent. We will demonstrate this in the next section.

To minhash a set, first pick a random permutation of the rows. Then, the minhash value is the value of the first row that has a 1, preserving the permuted order.

\begin{exmp}
In this example we will reuse the two sets of Example \ref{exmp:matrixRepresentation}. Suppose that the random permutation has given the following order: "di", "da", "ia", "na", "ad", "al". Let h be the minhash function.

\begin{center}
	\includegraphics[width=2in]{permutationMatrixSignature}
	\label{fig:permutationMatrixSignature}
	
	Figure 4: Permutation of the matrix of Example \ref{exmp:matrixRepresentation}.
\end{center}

In the first column, we can see that $h(S_1) = "da"$ and in the second one we see that $h(S_2) = "di"$.

\end{exmp}

With this technique, we can see that every time we do a permutation, we only occupy one new row of the "signature matrix", which is reducing a lot the space.

\subsubsection{Preserving the Jaccard Similarity}

As we mentioned before, the Jaccard Similarity in the matrix is equal to the number of rows of type \textit{d} divided by the number of rows of type \textit{b} + \textit{c} + \textit{d}. And that Similarity is preserved in the "signature matrix".

\begin{proof}
Look down through the permuted columns $C_1$ and $C_2$ until we see a 1 in any of the two columns. Then, we can have a row of type \textit{b} or \textit{c}, where only one of the columns have a 1, or a row of type \textit{d}, where both columns have a 1 in it.
If we find a row of type \textit{d}, then the minhash function will take the same row. Thus, $h(C_1) = h(C_2)$.
Otherwise, we must have a row of type \textit{b} or \textit{c} and $h(C_1) \neq h(C_2)$.

We can see that this is exactly the Jaccard Similarity. The probability of two columns have the same minhash value is equal to the number of rows of type \textit{d} divided by the number of rows of type \textit{b} + \textit{c} + \textit{d}.

\begin{equation}
P[h(C_1) = h(C_2)] = J(C_1, C_2)
\end{equation}

\end{proof}

\subsubsection{Optimizing the Time for Permutations}

Doing a set of permutation would be very expensive, but we can use a trick to simulate the \textit{n} permutations. We can use \textit{n} random hash functions that maps row numbers to as many buckets as the size of the rows of the matrix. We say that this is a trick because we are not doing a literal permutation, as we can have collisions using the hash functions but the point is that the probability of a collision with a large number \textit{k} for the shingling is very small. Thus, we can think of this new algorithm as a perfect permutation of the rows. The algorithm is the following:

\hfill

\begin{algorithmic}
%\Require $n \geq 0$
%\Ensure $y = x^n$
 \For{each row $r$}                    
  \For{each hash function $h_i$}                    
    \State {compute $h_i(r)$;}
  \EndFor
 \EndFor

 \For{each column $c$}                    
  \If{$c$ has 1 in row $r$}
    \For{each hash function $h_i$}                    
      \If{$h_i(r)$ is smaller than $M(i, c)$}
  	    \State $M(i, c) := h_I(r)$;
  	  \EndIf
    \EndFor
  \EndIf
 \EndFor
\end{algorithmic}

\hfill

Every time we go to the next row, we first compute all the hash values using the row value as their input. Then, for each position in that row, if there is a 1 we compare the hashed value of that position with the value in the signature matrix for all the hash functions computed. In case that this new value is smaller we change the value of the signature matrix by this new one. Doing this, in the end we have the smallest possible number in each position of the signature matrix.

\paragraph{Hashing Functions}
\label{par:hF}

We will present you now all the types of hashing functions we have used in the previous algorithm and in the algorithms in the Section \ref{sec:ALG}.

%A hash function is any function that converts a value to another value. We will fit this method in our algorithm to find the similarity between documents in the fastest way rather than making the calculus using the Jaccard Algorithm, computing a large number of hash functions all of the same type and then saving it in a matrix where we will know for each hash function and document wich is the smallest number that we have found.

%In this algorith we have used these hash functions:

\subparagraph{Modular Hashing}

\hfill

In this case we will recieve a key \textit{k}, a prime number \textit{p}, a random number modulus the number of shingles \textit{m} and the number of k-shingles \textit{z}.
The result will be a number modulus the number of k-shingles of the document.

modularHashFunction(k, p, m, z) = (k * p + m) mod z

\hfill

\begin{algorithmic}
 \Require Four integers $k, p, m, z$
 \Ensure An integer $h$
 \State{return $(k * p + m)$ mod $z$}
\end{algorithmic}

\subparagraph{Robert Jenkins' 96 bit Mix Function}

\hfill

In this case we will recieve a prime number \textit{p}, a random number modulus the number of shingles \textit{m} and a key \textit{k}.
We will compute the returning value \textit{h}, that will be found making a sequence of subtraction, exclusive-or, and bit shift.

mix(p, m, k) = h

\section{Locality Sensitive Hashing (LSH)}

We have talked before about how to improve the space complexity, but we still have not introduced a technique to improve the time complexity.

The general idea is that only a few pair of documents will have a high Jaccard Similarity. Thus, we need an algorithm that chooses the candidates that are more likely to be similar. The Locality Sensitive Hashing will help on this a lot.

The approach of LSH is to hash groups of rows into buckets in the sense that if two or more documents are hashed into the same bucket for at least one time, they will be very likely to be similar and will be put as a candidate pair.

\subsection{Band partition}

LSH will divide the signature matrix into \textit{b} bands of \textit{r} rows per band. Then, the algorithm will hash the set of rows that form a band for all the documents. Finally, it will create a new pair of candidates for every pair of documents hashed into the same bucket (for at least one time).

\begin{center}
	\includegraphics[width=6in]{BandPartition}
	\label{fig:bandPartition}
	
	Figure 5: Band partition of a signature matrix and hashing the values of the rows of the first band int othe buckets.
\end{center}

We can see that the set of rows of value "0143", from top to bottom, are hashed into the same bucket, and will be put as a candidate pair. However, the other two set of rows ("2130" and "1204") will not be hashed into the same bucket, and will not be put as a candidate pair (with this band).

\subsection{Choice of \textit{b} and \textit{r} values}

The election of \textit{b} and \textit{r} values is a very important part of the algorithm. See that the bigger \textit{r}, the less probable is to hash two sets of rows into the same bucket. This is because the probability that two sets of documents hashes into the same bucket decreases exponentialy each time we add one row into \textit{r}.

We want to get the least number of dissimilar documents that are hashed into the same bucket. These are called \textit{false positives}. On the other hand, we also want to minimize the number of similar documents that are hashed into different buckets. These other ones are called \textit{false negatives}.

The idea is to select a threshold, which is the minimum value that the Jaccard Similarity can have to consider two documents as similar, and then compute the \textit{b} and \textit{r} values.

According to Anand Rajaraman, Jure Leskovec,
and Jeffrey D. Ullman (2011), "a good aproximation to the threshold is $(1/b)^{1/r}$" (p. 90). So if we take a threshold \textit{t}, then we can calculate the \textit{b} and \textit{r} values as following:

Having that r = nhashFunctions/b,

\begin{equation}
\label{fun:br}
t \approx (\dfrac{1}{b})^{\dfrac{1}{r}} = (\dfrac{1}{b})^{\dfrac{b}{nhashFunctions}}
\end{equation}

Thus, using an arbitrary threshold, we can calculate the best approximate value of \textit{b} and \textit{r}.

\section{Algorithms}
\label{sec:ALG}

The objective in this section is to show the idea behind the algorithms we have programmed and give you the time complexity for each one. If you want to see the code, you will have to go to the algorithms folder.

\subsection{Jaccard Similarity}

The main objective in this section is to calculate the Jaccard Similarity between two documents in different ways.

\subsubsection{k-Shingles}

The idea is to create two sets, one per document, and insert all substrings of size \textit{k} of the documents. Afterwards, we will just have two make a division: the number of shingles in the intersection divided by the number of shingles in the union.

For each document:

\begin{algorithmic}
 \Require $k$
 \Ensure Returns an unordered\_set with all the substrings of size k of the document
 \State{\textit{words} := entire document}
 \State{\textit{pos} := 0}
 \State{unordered\_set \textit{S} := $\O$}
 \While {\textit{pos + k} $<=$ \textit{words.size()}}
	\State{\textit{sub} := substring from \textit{pos} to \textit{pos + (k - 1)}}
	\State{insert \textit{sub} into \textit{S}}
 \EndWhile
 \State{return \textit{S}}
\end{algorithmic}
\hfill
\hfill
Once we have calculated the \textit{k}-Shingles, we need to compute the intersection and the union of the sets. Note that we insert the substrings in an unordered set. This will be very usefull to improve the time complexity in the next two algorithms:
\hfill
\hfill
\begin{algorithmic}
 \Require Two sets $S_1$ and $S_2$
 \Ensure Returns the intersection set between $S_1$ and $S_2$
 \State{unordered\_set intersection := $\O$}
 \For{each element in $S_1$}                                       
  \If {$S_2$ contains the element in $S_1$}
	\State{insert the element into intersection}
  \EndIf
 \EndFor
 \State{return \textit{intersection}}
\end{algorithmic}
\hfill
\hfill
\begin{algorithmic}
 \Require Two sets $S_1$ and $S_2$
 \Ensure Returns the union set between $S_1$ and $S_2$
 \State{unordered\_set union := $S_1$}
 \For{each element in $S_2$}                                       
  \If {the element in $S_1$ is not contained in $S_1$}
	\State{insert the element into union}
  \EndIf
 \EndFor
 \State{return \textit{union}}
\end{algorithmic}

As you can see, we visit only one set in each algorithm. The good thing is that finding if an element belongs to an unordered set or not is $O(1)$. Thus, the time complexity for these two algorithms is $O(n)$, where \textit{n} is the size of the smallest unordered set\footnote{Note that we can change the set we are visiting by the other one. In the intersection, if $S_2$ is smaller than $S_1$, we can visit $S_2$. In the union, if $S_2$ is bigger than $S_1$, we can match the unordered set with $S_2$ and visit $S_1$.}, and the total time complexity is $O(n)$, as inserting elements in an unordered set is $O(1)$.

On the other hand, if we would have used the predefined functions \textit{set\_intersection} and \textit{set\_union}, we would have needed two ordered set, as it is a precondition of these two functions. Thus, the total cost would have been $O(n * log(n))$.

Finally, we just have to divide the size of the intersection set by the size of the union set (and multiply by 100 if we want a percentage).

\begin{equation}
Jsim(D_1, D_2) = \dfrac{intersection.size()}{union.size()} * 100
\end{equation}

\paragraph{Cost}

The cost of the first algorithm is $O(k * (t - k))$, where \textit{k} is the k-Shingle value and \textit{t} is the size of \textit{words}. As \textit{k} is always a constant, the final cost of this algorithm is $O(k * t - k^2) = O(t)$. The cost of the next two algorithms are $O(n)$, as we have said in the previous section. Finally, the cost of a division and a multiplication is $O(1)$.

Thus, the total cost of calculating the Jaccard Similarity using only k-Shingles is $O(n) + O(t) + O(1) = O(n)$, as usually the number of shingles is much larger than the size of \textit{words}.

\subsubsection{Minhash Signatures}

We know that implementing just k-Shingling is very expensive in terms of size and time (if we want to compare n documents between them, the complexity is $O(n^2)$). Implementing minhash signatures will help on this a lot. However, the Jaccard Similarity will not be the exact value. To do this, we will use as our input the k-Shingle sets calculated in the previous section.
\hfill
\hfill
\begin{algorithmic}
 \Require Two sets $S_1$ and $S_2$
 \Ensure Returns an approximate Jaccard Similarity value between $S_1$ and $S_2$
 \State{unordered\_set union = $S_1 U S_2$}
 \State{matrix signatures = infinity}
 \State{vector h = all the hash functions we will use}
 
  \For{each row $r$}                    
  \For{each hash function $h[i]$}                    
    \State {compute $h[i](r)$;}
  \EndFor
 \For{each column $c$}                    
  \If{$c$ has 1 in row $r$}
    \For{each hash function $h[i]$}                    
      \If{$h[i](r)$ is smaller than $signatures[i][c]$}
  	    \State $signatures[i][c] := h[i](r)$;
  	  \EndIf
    \EndFor
  \EndIf
 \EndFor
  \EndFor
 \State{intersection := 0}
 \State{doc1 := 0}
 \State{doc2 := 1}
 \For{each hash function $h[i]$}                    
    \If{$signatures[i][doc1] == signatures[i][doc2]$}
      \If{$signatures[i][doc1] != infinity$}
  	  	\State{++intersection}
  	  \EndIf
  	\EndIf
  \EndFor
 \State{return $\dfrac{intersection}{h.size()}$}
\end{algorithmic}

\hfill

We use the hash functions that we introduced in section \ref{par:hF}. We also have tried to use MurmurHash and Multiplicative Hash as hashing functions but we have had some problem when using them.

\paragraph{Cost}

We can deduce by just watching the code that the time complexity will come from the first for loop, as it is the one that has to compute most things.
Inside the loop, we have two more loops:

\begin{enumerate}
   \item This loop is responsible of calculating the hash functions taking as the input the row number. Let \textit{h} be the number of hash functions we are using. Then, the cost is $O(h)$ because the calculus done inside this loop is constant.
   
   \item This other loop is the one that updates (or not) the signature matrix. For every column \textit{c}:
   \begin{itemize}
     \item Watch if the set representing the document has the shingle of the row \textit{r}. The cost of doing this is $O(1)$.
     \item Compare the results of all the values computed in the previous for. If any value is smaller than the one that is in the signature matrix at position [\textit{h}][\textit{c}], just replace it by the new value. The cost of this for is $O(c * h)$, where \textit{c} is the number of documents and \textit{h} is the number of hash functions.
   \end{itemize}
\end{enumerate}

Thus, the cost of this function is $O(r * (h + c * (h + 1))) = O(r * h + r * c * h + r * c) = O(r * c * h)$, where \textit{r} is the number of rows and \textit{h} and \textit{c} the values explained before. We should point that \textit{h} is a constant value (usually 200 is correct), so the real cost is $O(r * c)$.

\subsection{LSH}

The first thing we need to do in this algorithm is finding the right values for \textit{b}, number of bands, and \textit{r}, number of rows per band depending on the value of a \textit{threshold}. To do so, we need an algorithm that calculates the \textit{b} value \footnote{It is also possible to calculate the \textit{r} value and later divide \textit{nhashFunctions} by \textit{r} to get the \textit{b} value, but it is more difficult to calculate it.} that has the minimum error with the threshold. The error is calculated as the function (\ref{fun:br}) minus the \textit{threshold}.

\hfill

\begin{algorithmic}
 \Require nhashFunctions = number of rows of the signature matrix, and a threshold
 \Ensure Returns the \textit{b} value
 
 \State{b := 1}
 \State{minError := 1.0}
 %\State{nhashFunctions := SM.size()}
 \For{each row $h$ of SM}
	\If{nhashFunctions mod h $==$ 0}
      \State{aux := $(1/h)^{(h/nhashFunctions)}$}
      \State{absolute := $\abs{aux - threshold}$}
      \If{absolute $<$ minError}
        \State{minError := absolute}
        \State{b := h}
  	  \EndIf
  	\EndIf
 \EndFor
 
 \State{return \textit{b}}
\end{algorithmic}
Once we have calculated the \textit{b} value, we just need to divide \textit{nhashFunctions} by \textit{b} to get the number of rows per band.

Now that we have the best possible values for \textit{b} and \textit{r}, it is time to begin with the LSH algorithm. This algorithm will hash each set of rows from each band and if two set of rows from the same band hashes to the same bucket, both documents will be inserted in a data structure of possible candidates.

\begin{algorithmic}
 \Require The signature matrix of the \textit{c} documents we have SM, \textit{b} and \textit{r}
 \Ensure Returns the condidate pairs
 
 \State{unordered\_map buckets := $\O$}
 \State{unordered\_map candidates := $\O$}

 \For{each band $b$}
    %\State{nb := b/r}
	\For{each document $c$}
	  \State{hv := hash value of SM[b][c]}
	  \State{buckets[hv].insert(c)}
    \EndFor
    
    \For{each element $val$ of buckets}
	  \For{each element $d$ of buckets[val]}
	  	\State{j := 1}
	  		\While {position of $d$ + j $<$ buckets[val].size()}
	  			\State{elem := element of buckets[val] in position ($d$ + j)}
	  			\If{absolute $<$ minError}
       				\State{insert elem into candidates[d]}
  				\Else
		 			\State{insert $d$ into candidates[elem]}
		 		\EndIf
 			\EndWhile
      \EndFor
    \EndFor
    \State{buckets.clear()}
 \EndFor
 \State{return \textit{candidates}}
\end{algorithmic}

\hfill

At this moment, we have the list of pairs of candidates that we want with the threshold \textit{t}.

The last thing we have to do is calculate the Jaccard Similarity between all the pairs of candidates of the list. As we have done an aproximation when calculating the values \textit{b} and \textit{r}, it is very probable that some pair of candidates have less Jaccard Similarity than \textit{t}.

\subsubsection{Cost}

The LSH algorithm has two parts. The first one calculates the best possible values of \textit{b} and \textit{r}. The second one chooses the candidate pairs of documents.

\begin{enumerate}
   \item The cost of this part is $O(h)$, where \textit{h} is the total number of hash functions used in the signature matrix (\textit{h} is the number of rows of the signature matrix).
   
   \item Let $c$ be the number of documents and $b$ the number of bands. The cost of this part of the algorithm is $O(c * b^2)$. For each band \textit{b}:
   \begin{itemize}
     \item For each document compute the hash value of each set of rows of band \textit{b}. The cost of doing this is $O(c)$, as the computational cost of calculating the hash values is $O(1)$.
     
     \item For each non-empty position of the buckets, create pairs of candidates between all the documents of the bucket without repetitions. This means that documents 3 and 4 will only have one candidate pair (3, 4), and not two (3, 4) and (4, 3). The cost of this for loop is $O(c^2)$, as in the worst case all the documents will be in the same bucket and we will have to make $(c * (c - 1))/2$ candidate pairs.
   \end{itemize}
\end{enumerate}

\section{Plots}

All plots have been done using Python. For the experiments, we have choose random pairs of documents of 50 words permuted randomly. Each plot has been done using the same 5 pairs of documents (chosen randomly) as samples.

\subsection{Jaccard Similarity}

\subsubsection{k-Shingles}

The following plot shows the evolution of the Jaccard Similarity when we vary the value of \textit{k}.

\begin{center}
	\includegraphics[width=5in]{jsim_k_sim.png}
	\label{fig:jsim_k_sim.png}
	
	\textit{Figure 6: Jaccard Similarity varying the \textit{k} value.}
\end{center}

As you can see, the Jaccard Similarity of the different pairs is very similar because all the documents are a random permutation of the same 50 words. You can see that because when the shingle size is equal to 1 (we only take the characters), the Jaccard Similarity is 100\%\footnote{Note that in a document with few words, it is very probable that not all the characters are included in the document.}.

Besides, it is important to remark the direction of the graphic. The bigger the shingle size, the lower Jaccard Similarity we get. This is because every time we increase the \textit{k} value, it is more likely that the substrings start to take some characters of the next word. Thus, the probability that two different documents have the same shingles decreases a lot. You can see that when the shingle size is bigger or equal than 6, we begin to have a very low Jaccard Similarity (less than 20 from \textit{k} = 7) because most of the substrings will have characters of the next words.

Now, we are going to represent the time cost of the same pair of documents when we vary the shingle size.

\begin{figure}[!htb]
   \begin{minipage}{0.48\textwidth}
     \centering
     \includegraphics[width=.9\linewidth]{jsim_k_time.png}
     \textit{Figure 7: Time to compute the Jaccard Similarity when varying the \textit{k} value.}\label{Fig:jsim_k_time.png}
   \end{minipage}\hfill
   \begin{minipage}{0.48\textwidth}
     \centering
     \includegraphics[width=.9\linewidth]{jsim_k_meanTime.png}
     \textit{Figure 8: Average time to compute the Jaccard Similarity of the Figure 7.}
     \label{Fig:jsim_k_meanTime.png}
   \end{minipage}
\end{figure}

If we focus on the time cost we can see that the time does not vary at all when we use different shingle sizes. This may be because the documents have the same number of words. Thus, the number of shingles for each document must be very similar to the others.

We should also point that the time to compute the Jaccard Similarity of these pairs of documents is very small because the number of words is very small (50 words are very few compared to the number of words of an article).

\begin{center}
	\includegraphics[width=5in]{jsim_spaces_k_sim.png}
	\label{fig:jsim_spaces_k_sim.png}
	
	\textit{Figure 9: Time to compute the Jaccard Similarity when varying the \textit{k} value.}
\end{center}

In the following plot we can see the differences between the behaviour of the algorithm taking into account the spaces or not.
The blue line (coutning spaces) shows that the Jaccard Similarity calculated counting the spaces in the documents is higher than not counting them (orange line). This is due to the difference between the k-shingles created in both algorithms, because if we consider the spaces then we will need a higher \textit{k} for the substring to start taking characters from the next word.

From now on, we will use spaces to experiment with the documents in the following plots.

We also have put the option to make all characters to lowercase or not. This does not affect on these documents, as all the characters are already in lowercase.

\subsubsection{Signature Matrix}

\paragraph{Using Modular Hashing}

In the following plot, it is represented an approximation of the result of calculating the Jaccard Similarity using the signature matrix.

\begin{center}
	\includegraphics[width=5in]{signature_nHash_k_meanSim.png}
	\label{fig:signature_nHash_k_meanSim.png}
	
	\textit{Figure 10: Jaccard Similarity aproximation varying the shingle sizes.}
\end{center}

 The similarity between the documents is strictly related with the number of hashing functions that we use. When the number of hashing functions is a small value it is very different than the Jaccard Similarity, because the signature matrix has few rows and then, there are few chances that one row has the same hash values on two different columns. But when the number of hash functions increases, the functions are more alike to the Jaccard Similarity (Figure 6).
 
 See that when the nHashes is 1, the Jaccard Similarity should only be 100 or 0 per cent, as there is only one row in the signature matrix, but remember that this plot is using the mean value of the result of computing the Jaccard Similarity of 5 pairs using the signature matrix.

Now, we are going to focus on the number of hash functions.

\begin{figure}[!htb]
   \begin{minipage}{0.48\textwidth}
     \centering
     \includegraphics[width=.9\linewidth]{signature_nHash_k_meanSim3D.png}
     \textit{Figure 11: Jaccard Similarity approximation varying the number of hash function and the size of the shingles.}\label{Fig:signature_nHash_k_meanSim3D.png}
   \end{minipage}\hfill
   \begin{minipage}{0.48\textwidth}
     \centering
     \includegraphics[width=1.08\linewidth]{signature_nHash_k_meanTime.png}
     \textit{Figure 12: Average time to compute the Jaccard Similarity approximation varying the shingle sizes.}\label{Fig:signature_nHash_k_meanTime.png}
   \end{minipage}
\end{figure}

We can see in Figure 11 that the plot form begins to stay stable from number of hash functions equal to 200 more or less. Furthermore, in Figure 12 the time cost starts to increase a lot from number of hash function equal to 250.

Thus, even though using more number of hash function would be better to calculate an approximation of the Jaccard Similarity, it is not usefull to do so because with 200 hash functions we compute a very similar value in less time.

It is important to remark that in Figure 12 the time stabilize near the \textit{k} equal to the average word length.

\hfill

In Figures 13, 14 and 15 there is represented how much is the error value obtained using the Jaccard Similarity and the Signature Matrix Approximation. Approximately we can arrive to the same conclusions as the ones that are before. The more number of hash functions, the less the error is.

\begin{figure}[!htb]
   \begin{minipage}{0.48\textwidth}
     \centering
     \includegraphics[width=.9\linewidth]{signature_jsim_meanError.png}
     \caption{Figure 13: Error between the Jaccard Similarity and the Jaccard Similarity approximated in the signature matrix.}\label{Fig:signature_jsim_meanError.png}
   \end{minipage}\hfill
   \begin{minipage}{0.48\textwidth}
     \centering
     \includegraphics[width=0.9\linewidth]{signature_jsim_meanError_from25.png}
     \textit{Figure 14: The same than Figure 13 but starting from nHashes = 25.}\label{Fig:signature_jsim_meanError_from25.png}
   \end{minipage}
\end{figure}

\begin{figure}
\centering
     \includegraphics[width=1\linewidth]{signature_jsim_meanError_from50.png}
     \textit{Figure 15: The same than Figure 13 but starting from nHashes = 50.}\label{Fig:signature_jsim_meanError_from50.png}
\end{figure}

\break

We have put the same plot three times so that you can see more closely the values of the mean error.

\paragraph{Using Robert Jenkins' 96 bit Mix Function}

We will show the same plots shown in the previous section. If we do not say anything about any plot, it means that it has the same description as the previous section.

\begin{center}
	\includegraphics[width=5in]{signature_nHash_k_meanSim_jenkinsFunc.png}
	\label{fig:signature_nHash_k_meanSim_jenkinsFunc.png}
	
	\textit{Figure 16: Jaccard Similarity aproximation varying the \textit{k} value.}
\end{center}

\begin{figure}[!htb]
   \begin{minipage}{0.48\textwidth}
     \centering
     \includegraphics[width=.9\linewidth]{signature_jsim_meanError_jenkinsFunc.png}
     \textit{Figure 17: Time to compute the Jaccard Similarity aproximation when varying the \textit{k} value.}\label{Fig:signature_jsim_meanError_jenkinsFunc.png}
   \end{minipage}\hfill
   \begin{minipage}{0.48\textwidth}
     \centering
     \includegraphics[width=.9\linewidth]{signature_nHash_k_meanTime_jenkinsFunc.png}
     \textit{Figure 18: Average time to compute the Jaccard Similarity aproximation varying the shingles sizes.}\label{Fig:jsim_k_meanTime.png}
   \end{minipage}
\end{figure}

\begin{figure}[!htb]
   \begin{minipage}{0.48\textwidth}
     \centering
     \includegraphics[width=.9\linewidth]{signature_jsim_meanError.png}
     \textit{Figure 19: Error between the Jaccard Similarity and the Jaccard Similarity approximated in the signature matrix.}\label{Fig:signature_jsim_meanError.png}
   \end{minipage}\hfill
   \begin{minipage}{0.48\textwidth}
     \centering
     \includegraphics[width=0.9\linewidth]{signature_jsim_meanError_from25_jenkinsFunc.png}
     \textit{Figure 20: The same than Figure 19 but starting from nHashes = 25.}\label{Fig:signature_jsim_meanError_from25_jenkinsFunc.png}
   \end{minipage}
\end{figure}

\begin{figure}
\centering
     \includegraphics[width=1\linewidth]{signature_jsim_meanError_from25_jenkinsFunc.png}
     \textit{Figure 21: The same than Figure 19 but starting from nHashes = 50.}\label{Fig:signature_jsim_meanError_from25_jenkinsFunc.png}
\end{figure}

\break
\break

The only difference we should say is that in the first plot, whe the number of hash functions is 1, the function is closer to the rest of number of hash functions, which is what we want.

\break

\paragraph{Modular Hashing vs. Robert Jenkins' 96 bit Mix Function}

The following plots represent the error using the modular hashing minus the error using the jenkins hashing. If the line is in the positives Y, it means that the error using the modular hashing is greater than the error using the jenkins hashing, and viceversa.

\begin{figure}[!htb]
   \begin{minipage}{0.48\textwidth}
     \centering
     \includegraphics[width=.9\linewidth]{sig_error_modular_vs_jenkinsFunc.png}
     \textit{Figure 22: Diference between the error using the modular hashing and the one using the jenkins hashing.}\label{Fig:sig_error_modular_vs_jenkinsFunc.png}
   \end{minipage}\hfill
   \begin{minipage}{0.48\textwidth}
     \centering
     \includegraphics[width=0.9\linewidth]{sig_error_modular_vs_jenkinsFunc_from25.png}
     \textit{Figure 23: The same than Figure 22 but starting from nHashes = 25.}\label{Fig:sig_error_modular_vs_jenkinsFunc_from25.png}
   \end{minipage}
\end{figure}

\begin{figure}
\centering
     \includegraphics[width=1\linewidth]{sig_error_modular_vs_jenkinsFunc_from50.png}
     \textit{Figure 24: The same than Figure 22 but starting from nHashes = 50.}\label{Fig:sig_error_modular_vs_jenkinsFunc_from50.png}
\end{figure}

\break

We can see that the error using the modular hashing in Figures 22, 23 and 24 is greater most of the time, specially at the begining. Thus, the Jaccard Similarity approximation using the jenkins hashing is better than the one using the modular hashing.

In the Figure 25 we have represented the time cost using modular and jenkins hashing. If we are in the positive Y, the modular functions spends more time on the algorithm. But we see that it is the contrary: the jenkins hashing is the one that spends more time even though it has less error than the modular hashing.
 
\begin{center}
     \includegraphics[width=1\linewidth]{sig_time_modular_vs_jenkinsFunc.png}
     \textit{Figure 25: Difference of the time between the modular function and the jenkins function.}\label{Fig:sig_time_modular_vs_jenkinsFunc.png}
\end{center}

\subsection{LSH}

For these set of plots, we are going to use 500 hash function, even tough we know that 200 would be enough.

The following plot shows the mean time spent running the LSH algorithm and also the time that we need to compute all the candidate pairs without using LSH.

\begin{center}
     \includegraphics[width=1\linewidth]{lhs_time.png}
     \textit{Figure 26: Difference of the time between the modular function and the jenkins function.}\label{Fig:lhs_time.png}
\end{center}

The ideal plot would be the one where all the lines are below the discontinuous one. Then, using any value for the threshold, we could compute the copied pairs using LSH faster than without using it.

However, we see that this is not happening in the plot. This is because 20 documents are not big enough to be faster using LSH.

Following this, there are plots shown to ilustrate the LSH algorithm performance according to various thresholds.

\begin{center}
\centering
     \includegraphics[width=1\linewidth]{lsh_t0.png}
     \textit{Figure 27: Average number of candidate pairs, number of copy pairs detected, false positives and false negatives with threshold = 0.0.}\label{Fig:lsh_t0.png}
\end{center}

\begin{center}
\centering
     \includegraphics[width=1\linewidth]{lsh_t02.png}
     \textit{Figure 28: Average number of candidate pairs, number of copy pairs detected, false positives and false negatives with threshold = 0.2.}\label{Fig:lsh_t02.png}
\end{center}

\begin{center}
\centering
     \includegraphics[width=1\linewidth]{lsh_t042.png}
     \textit{Figure 29: Average number of candidate pairs, number of copy pairs detected, false positives and false negatives with threshold = 0.42.}\label{Fig:lsh_t042.png}
\end{center}

\begin{center}
\centering
     \includegraphics[width=1\linewidth]{lsh_t064.png}
     \textit{Figure 30: Average number of candidate pairs, number of copy pairs detected, false positives and false negatives with threshold = 0.64.}\label{Fig:lsh_t064.png}
\end{center}

\begin{center}
\centering
     \includegraphics[width=1\linewidth]{lsh_t08.png}
     \textit{Figure 31: Average number of candidate pairs, number of copy pairs detected, false positives and false negatives with threshold = 0.80.}\label{Fig:lsh_t080.png}
\end{center}

\begin{center}
\centering
     \includegraphics[width=1\linewidth]{lsh_t10.png}
     \textit{Figure 32: Average number of candidate pairs, number of copy pairs detected, false positives and false negatives with threshold = 1.0.}\label{Fig:lsh_t10.png}
\end{center}

We can see that the higher threshold value, the lower the shingle size has to be, in order for pairs to be detected as copies. In general, we start to see a decrease in copies detected from shingle size = 5, which could be related to the fact that the average word size is 5.84.

\break

% Bibliografía.
%-----------------------------------------------------------------
\newpage
\begin{thebibliography}{9}
\bibitem{The Best} 
Anand Rajaraman, Jure Leskovec and Jeffrey D. Ullman. 
\textit{Mining of Massive Datasets}. Cambridge University Press. (December 30, 2011).
 
 \bibitem{Bottom} 
 cmhteixeira. \textit{Locality Sensitive Hashing (LSH)} [online]. (November 29, 2017). $<$https://aerodatablog.wordpress.com/2017/11/29/locality-sensitive-hashing-lsh/$>$[Consulted: December 12, 2018].
 
 \bibitem{Wtf is this name}
 Hubert Brylkowski. \textit{Locality sensitive hashing $-$LSH explained} [online]. (October 6, 2017). $<$https://medium.com/engineering-brainly/locality-sensitive-hashing-explained-304eb39291e4$>$[Consulted: December 15, 2018].
 
 \bibitem{Ma maaan}
 Jeffrey D. Ullman[Mining Massive Datasets]. (July 23, 2016). \textit{3 2 Minhashing 25 18}. $<$https://www.youtube.com/watch?v=96WOGPUgMfw$>$.
 
 \bibitem{Ma man agaaaain}
 Jeffrey D. Ullman[Mining Massive Datasets]. (July 23, 2016). \textit{3 3 Locality Sensitive Hashing 19 24}. $<$https://www.youtube.com/watch?v=\_1D35bN95Go$>$.
 
 \bibitem{Second one}
 santhoshhari. \textit{Locality Sensitive Hashing: Application of Locality Sensitive Hashing to Audio Fingerprinting} [online]. (n. d.). $<$https://santhoshhari.github.io/Locality-Sensitive-Hashing/$>$[Consulted: December 16, 2018].
 
 \bibitem{The firsty one}
  Shikhar Gupta. \textit{Locality Sensitive Hashing: An effective way of reducing the dimensionality of your data} [online]. (June 29, 2018). $<$https://towardsdatascience.com/understanding-locality-sensitive-hashing-49f6d1f6134$>$[Consulted: December 16, 2018]
 
%\url{http://www.mit.edu/~andoni/LSH/} 
 
\end{thebibliography}

\end{document}